{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation\n",
    "\n",
    "Max pooling reduces the width and height of given volumes.\n",
    "\n",
    "\n",
    "| Symbols                   | Meaning               | Size                                              |\n",
    "|:--------------------------|:----------------------|:--------------------------------------------------|\n",
    "| $\\boldsymbol{X}$          | 2D Input Matrix       | $(H_{in}$, $W_{in})$                              |\n",
    "| $\\boldsymbol{\\mathsf{X}}$ | 3D Input Tensor       | $(H_{in}$, $W_{in}$, $C_{in})$                    |\n",
    "| $\\boldsymbol{\\mathsf{X}}$ | 4D Input Tensor       | $(N_{batch}$, $H_{in}$, $W_{in}$, $C_{in})$       |\n",
    "| $\\boldsymbol{Y}$          | 2D Output Matrix      | $(H_{out}$, $W_{out})$                            |\n",
    "| $\\boldsymbol{\\mathsf{Y}}$ | 3D Output Tensor      | $(H_{out}$, $W_{out}$, $C_{out})$                 |\n",
    "| $\\boldsymbol{\\mathsf{Y}}$ | 4D Output Tensor      | $(N_{batch}$, $H_{out}$, $W_{out}$, $C_{out})$    |\n",
    "\n",
    "As opposed to convolution, max pooling doesn't have any learnable parameters,\n",
    "so there's no $\\boldsymbol{\\mathsf{W}}$ or $\\boldsymbol{b}$ in the list of notation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08-2: Max Pooling Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calculation with NumPy and TensorFlow for the followings\n",
    "\n",
    "#### 1. Max Pooling with stride\n",
    "$(4 \\times 4) \\rightarrow (2 \\times 2)$ where $S=2$\n",
    "\n",
    "#### 2. Multiple Channels\n",
    "$(4 \\times 4 \\times 3) \\rightarrow (2 \\times 2 \\times 3)$ where $S=2$\n",
    "\n",
    "#### 3. Mini-batch\n",
    "$(4 \\times 4 \\times 4 \\times 3) \\rightarrow (4 \\times 2 \\times 2 \\times 3)$ where $S=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def float_sequence(size):\n",
    "    return np.arange(size, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Max Pooling with stride backward\n",
    "$(4 \\times 4) \\rightarrow (2 \\times 2)$ where $S=2$\n",
    "\n",
    "TensorFlow: [tf.nn.max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
    "\n",
    "For the size of output width and height, the same fomula as convolution operation hold.\n",
    "\n",
    "$$\n",
    "H_{out} = \\frac{H_{in} + 2P - H_{filter}}{S} + 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{out} = \\frac{W_{in} + 2P - F_{filter}}{S} + 1\n",
    "$$\n",
    "\n",
    "However, padding $P$ is not used in max pooling, and often filter size $H_{filter}$ and $F_{filter}$ is the same as stride $S$.\n",
    "As a result, the size can be calculated as follwing.\n",
    "\n",
    "$$\n",
    "H_{out} = \\frac{H_{in}}{S}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_{out} = \\frac{W_{in}}{S}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== X ===\n",
      "[[  0.   1.   2.   3.]\n",
      " [  4.   5.   6.   7.]\n",
      " [  8.   9.  10.  11.]\n",
      " [ 12.  13.  14.  15.]]\n",
      "=== dX ===\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  2.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  3.  0.  4.]]\n",
      "=== tf_dX ===\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  2.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  3.  0.  4.]]\n",
      "=== Matched? ===\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "W = float_sequence(2*2).reshape(2,2) + 1\n",
    "X = float_sequence(4*4).reshape(4,4)\n",
    "S = 2\n",
    "\n",
    "H_out = 4 // S\n",
    "W_out = 4 // S\n",
    "\n",
    "print(\"=== X ===\")\n",
    "print(X)\n",
    "\n",
    "dY = W\n",
    "\n",
    "dX = np.zeros((4, 4))\n",
    "for h in range(W_out):\n",
    "    for w in range(W_out):\n",
    "        h_start = h * S\n",
    "        h_end   = h_start + S\n",
    "        w_start = w * S\n",
    "        w_end   = w_start + S\n",
    "\n",
    "        current_dY = dY[h,w]\n",
    "\n",
    "        X_slice = X[h_start:h_end, w_start:w_end]\n",
    "        flat_X_slice = X_slice.reshape(-1)\n",
    "        max_index = np.argmax(flat_X_slice)\n",
    "\n",
    "        gradient = np.zeros_like(flat_X_slice)\n",
    "        gradient[max_index] = current_dY\n",
    "        gradient = gradient.reshape(X_slice.shape)\n",
    "        \n",
    "#         print(\"gradient\")\n",
    "#         print(gradient)\n",
    "                \n",
    "        dX[h_start:h_end, w_start:w_end] = gradient\n",
    "\n",
    "print(\"=== dX ===\")     \n",
    "print(dX)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf_X = tf.constant(X.reshape(1, 4, 4, 1))\n",
    "    tf_Y = tf.nn.max_pool(\n",
    "        tf_X,\n",
    "        ksize=[1, S, S, 1],\n",
    "        strides=[1, S, S, 1],\n",
    "        padding='VALID'\n",
    "    ) * tf.constant(W.reshape(1, 2, 2, 1))\n",
    "    tf_L = tf.reduce_sum(tf_Y)\n",
    "    tf_grad = tf.gradients(tf_L, [tf_X])\n",
    "    tf_dX, = sess.run(tf_grad)\n",
    "    print(\"=== tf_dX ===\")     \n",
    "    print(tf_dX[0, :, :, 0])\n",
    "\n",
    "print(\"=== Matched? ===\")    \n",
    "print(np.all(dX == tf_dX[0, :, :, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Channels backward\n",
    "$(4 \\times 4 \\times 3) \\rightarrow (2 \\times 2 \\times 3)$ where $S=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== dX ===\n",
      "[[[  0.   0.   0.   0.]\n",
      "  [  0.   1.   0.   4.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   7.   0.  10.]]\n",
      "\n",
      " [[  0.   0.   0.   0.]\n",
      "  [  0.   2.   0.   5.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   8.   0.  11.]]\n",
      "\n",
      " [[  0.   0.   0.   0.]\n",
      "  [  0.   3.   0.   6.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   9.   0.  12.]]]\n",
      "=== tf_dX ===\n",
      "[[[  0.   0.   0.   0.]\n",
      "  [  0.   1.   0.   4.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   7.   0.  10.]]\n",
      "\n",
      " [[  0.   0.   0.   0.]\n",
      "  [  0.   2.   0.   5.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   8.   0.  11.]]\n",
      "\n",
      " [[  0.   0.   0.   0.]\n",
      "  [  0.   3.   0.   6.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   9.   0.  12.]]]\n",
      "=== Matched? ===\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "W = float_sequence(2*2*3).reshape(2,2,3) + 1\n",
    "X = float_sequence(4*4*3).reshape(4,4,3)\n",
    "\n",
    "S = 2\n",
    "\n",
    "H_out = 4 // S\n",
    "W_out = 4 // S\n",
    "\n",
    "# print(\"=== X ===\")\n",
    "# print(X.transpose(2, 0, 1))\n",
    "\n",
    "dY = W\n",
    "dX = np.zeros((4, 4, 3))\n",
    "\n",
    "for h in range(W_out):\n",
    "    for w in range(W_out):\n",
    "        h_start = h * S\n",
    "        h_end   = h_start + S\n",
    "        w_start = w * S\n",
    "        w_end   = w_start + S\n",
    "        \n",
    "        current_dY = dY[h,w,:]\n",
    "#         print(\"====current_dY\")\n",
    "#         print(current_dY)\n",
    "\n",
    "        X_slice = X[h_start:h_end, w_start:w_end, :]\n",
    "#         print(\"====X_slice (raw)\")\n",
    "#         print(X_slice)\n",
    "#         print(\"====X_slice (by channel)\")\n",
    "#         print(X_slice.transpose(2, 0, 1))\n",
    "        \n",
    "        flat_X_slice_by_channel = X_slice.transpose(2, 0, 1).reshape(3, -1)\n",
    "#         print(\"====flat_X_slice_by_channel\")\n",
    "#         print(flat_X_slice_by_channel)\n",
    "        max_index = np.argmax(flat_X_slice_by_channel, axis=1)\n",
    "#         print(\"====max_index\")\n",
    "#         print(max_index)\n",
    "\n",
    "        gradient = np.zeros_like(flat_X_slice_by_channel)\n",
    "        gradient[np.arange(3), max_index] = current_dY\n",
    "#         print(\"gradient\")\n",
    "#         print(gradient)\n",
    "\n",
    "        gradient = gradient.reshape(X_slice.shape[2], X_slice.shape[0], X_slice.shape[1]).transpose(1, 2, 0)\n",
    "        \n",
    "#         print(\"gradient\")\n",
    "#         print(gradient)\n",
    "                \n",
    "        dX[h_start:h_end, w_start:w_end, :] = gradient\n",
    "\n",
    "\n",
    "print(\"=== dX ===\")     \n",
    "print(dX.transpose(2, 0, 1))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf_X = tf.constant(X.reshape(1, 4, 4, 3))\n",
    "    tf_Y = tf.nn.max_pool(\n",
    "        tf_X,\n",
    "        ksize=[1, S, S, 1],\n",
    "        strides=[1, S, S, 1],\n",
    "        padding='VALID'\n",
    "    ) * tf.constant(W.reshape(1, 2, 2, 3))\n",
    "    tf_L = tf.reduce_sum(tf_Y)\n",
    "    tf_grad = tf.gradients(tf_L, [tf_X])\n",
    "    tf_dX, = sess.run(tf_grad)\n",
    "    print(\"=== tf_dX ===\")     \n",
    "    print(tf_dX[0, :, :, :].transpose(2, 0, 1))\n",
    "\n",
    "print(\"=== Matched? ===\")    \n",
    "print(np.all(dX == tf_dX[0, :, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mini-batch backward\n",
    "$(4 \\times 4 \\times 4 \\times 3) \\rightarrow (4 \\times 2 \\times 2 \\times 3)$ where $S=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== dX (first) ===\n",
      "[[[  0.   0.   0.   0.]\n",
      "  [  0.   1.   0.   4.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   7.   0.  10.]]\n",
      "\n",
      " [[  0.   0.   0.   0.]\n",
      "  [  0.   2.   0.   5.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   8.   0.  11.]]\n",
      "\n",
      " [[  0.   0.   0.   0.]\n",
      "  [  0.   3.   0.   6.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   9.   0.  12.]]]\n",
      "=== tf_dX (first) ===\n",
      "[[[  0.   0.   0.   0.]\n",
      "  [  0.   1.   0.   4.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   7.   0.  10.]]\n",
      "\n",
      " [[  0.   0.   0.   0.]\n",
      "  [  0.   2.   0.   5.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   8.   0.  11.]]\n",
      "\n",
      " [[  0.   0.   0.   0.]\n",
      "  [  0.   3.   0.   6.]\n",
      "  [  0.   0.   0.   0.]\n",
      "  [  0.   9.   0.  12.]]]\n",
      "=== Matched? ===\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "W = float_sequence(4*2*2*3).reshape(4,2,2,3) + 1\n",
    "X = float_sequence(4*4*4*3).reshape(4,4,4,3)\n",
    "\n",
    "S = 2\n",
    "\n",
    "H_out = 4 // S\n",
    "W_out = 4 // S\n",
    "\n",
    "# print(\"=== X (first) ===\")\n",
    "# print(X[0, :, :, :].transpose(2, 0, 1))\n",
    "\n",
    "dY = W\n",
    "dX = np.zeros((4, 4, 4, 3))\n",
    "\n",
    "for n_batch in range(4):\n",
    "    for h in range(W_out):\n",
    "        for w in range(W_out):\n",
    "            h_start = h * S\n",
    "            h_end   = h_start + S\n",
    "            w_start = w * S\n",
    "            w_end   = w_start + S\n",
    "\n",
    "            current_dY = dY[n_batch, h, w, :]\n",
    "\n",
    "            X_slice = X[n_batch, h_start:h_end, w_start:w_end, :]\n",
    "            flat_X_slice_by_channel = X_slice.transpose(2, 0, 1).reshape(3, -1)\n",
    "            max_index = np.argmax(flat_X_slice_by_channel, axis=1)\n",
    "\n",
    "            gradient = np.zeros_like(flat_X_slice_by_channel)\n",
    "            gradient[np.arange(3), max_index] = current_dY\n",
    "            gradient = gradient.reshape(X_slice.shape[2], X_slice.shape[0], X_slice.shape[1]).transpose(1, 2, 0)\n",
    "                \n",
    "            dX[n_batch, h_start:h_end, w_start:w_end, :] = gradient\n",
    "            \n",
    "\n",
    "print(\"=== dX (first) ===\")     \n",
    "print(dX[0, :, :, :].transpose(2, 0, 1))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf_X = tf.constant(X.reshape(4, 4, 4, 3))\n",
    "    tf_Y = tf.nn.max_pool(\n",
    "        tf_X,\n",
    "        ksize=[1, S, S, 1],\n",
    "        strides=[1, S, S, 1],\n",
    "        padding='VALID'\n",
    "    ) * tf.constant(W.reshape(4, 2, 2, 3))\n",
    "    tf_L = tf.reduce_sum(tf_Y)\n",
    "    tf_grad = tf.gradients(tf_L, [tf_X])\n",
    "    tf_dX, = sess.run(tf_grad)\n",
    "    print(\"=== tf_dX (first) ===\")     \n",
    "    print(tf_dX[0, :, :, :].transpose(2, 0, 1))\n",
    "\n",
    "print(\"=== Matched? ===\")    \n",
    "print(np.all(dX == tf_dX[:, :, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized naive max pooling backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool_naive_backward(dY, X, S):\n",
    "    N_batch, H_in, W_in, C_in = X.shape\n",
    "    \n",
    "    H_out = H_in // S\n",
    "    W_out = W_in // S\n",
    "    \n",
    "    dX = np.zeros_like(X)\n",
    "\n",
    "    for n_batch in range(N_batch):\n",
    "        for h in range(H_out):\n",
    "            for w in range(W_out):\n",
    "                h_start = h * S\n",
    "                h_end   = h_start + S\n",
    "                w_start = w * S\n",
    "                w_end   = w_start + S\n",
    "\n",
    "                \n",
    "                current_dY = dY[n_batch, h,w,:]\n",
    "\n",
    "                X_slice = X[n_batch, h_start:h_end, w_start:w_end, :]\n",
    "                flat_X_slice_by_channel = X_slice.transpose(2, 0, 1).reshape(C_in, -1)\n",
    "                max_index = np.argmax(flat_X_slice_by_channel, axis=1)\n",
    "\n",
    "                gradient = np.zeros_like(flat_X_slice_by_channel)\n",
    "                gradient[np.arange(C_in), max_index] = current_dY\n",
    "                gradient = gradient.reshape(X_slice.shape[2], X_slice.shape[0], X_slice.shape[1]).transpose(1, 2, 0)\n",
    "\n",
    "                dX[n_batch, h_start:h_end, w_start:w_end, :] = gradient\n",
    "\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Matched? ===\n",
      "True 0.0\n"
     ]
    }
   ],
   "source": [
    "S = 2\n",
    "W = float_sequence(10*4*4*3).reshape(10,4,4,3) + 1\n",
    "X = np.random.randn(10, 8, 8, 3).astype(np.float32) * 45676.362342398\n",
    "dY = W\n",
    "\n",
    "dX = max_pool_naive_backward(dY, X, S)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf_X = tf.constant(X)\n",
    "    tf_Y = tf.nn.max_pool(\n",
    "        tf_X,\n",
    "        ksize=[1, S, S, 1],\n",
    "        strides=[1, S, S, 1],\n",
    "        padding='VALID'\n",
    "    ) * tf.constant(W)\n",
    "    tf_L = tf.reduce_sum(tf_Y)\n",
    "    tf_grad = tf.gradients(tf_L, [tf_X])\n",
    "    tf_dX, = sess.run(tf_grad)\n",
    "\n",
    "print(\"=== Matched? ===\")    \n",
    "check = np.linalg.norm(dX - tf_dX) / ((np.linalg.norm(dX) + np.linalg.norm(tf_dX)))\n",
    "print(check < 1e-7, check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Matched? ===\n",
      "True 0.0\n"
     ]
    }
   ],
   "source": [
    "S = 2\n",
    "X = np.random.randn(128, 28, 28, 3).astype(np.float32)\n",
    "dY = np.ones((128, 14, 14, 3))\n",
    "\n",
    "dX = max_pool_naive_backward(dY, X, S)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf_X = tf.constant(X)\n",
    "    tf_Y = tf.nn.max_pool(\n",
    "        tf_X,\n",
    "        ksize=[1, S, S, 1],\n",
    "        strides=[1, S, S, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    tf_L = tf.reduce_sum(tf_Y)\n",
    "    tf_grad = tf.gradients(tf_L, [tf_X])\n",
    "    tf_dX, = sess.run(tf_grad)\n",
    "\n",
    "print(\"=== Matched? ===\")    \n",
    "check = np.linalg.norm(dX - tf_dX) / ((np.linalg.norm(dX) + np.linalg.norm(tf_dX)))\n",
    "print(check < 1e-7, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 ms ± 2.66 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n3 -r3\n",
    "\n",
    "max_pool_naive_backward(dY, X, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Matched? ===\n",
      "True 0.0\n"
     ]
    }
   ],
   "source": [
    "S = 3\n",
    "X = np.random.randn(128, 30, 12, 6).astype(np.float32)\n",
    "dY = np.ones((128, 10, 4, 6))\n",
    "\n",
    "dX = max_pool_naive_backward(dY, X, S)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf_X = tf.constant(X)\n",
    "    tf_Y = tf.nn.max_pool(\n",
    "        tf_X,\n",
    "        ksize=[1, S, S, 1],\n",
    "        strides=[1, S, S, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    tf_L = tf.reduce_sum(tf_Y)\n",
    "    tf_grad = tf.gradients(tf_L, [tf_X])\n",
    "    tf_dX, = sess.run(tf_grad)\n",
    "\n",
    "print(\"=== Matched? ===\")    \n",
    "check = np.linalg.norm(dX - tf_dX) / ((np.linalg.norm(dX) + np.linalg.norm(tf_dX)))\n",
    "print(check < 1e-7, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Matched? ===\n",
      "True 0.0\n"
     ]
    }
   ],
   "source": [
    "S = 5\n",
    "X = np.random.randn(1, 100, 200, 16).astype(np.float32)\n",
    "dY = np.ones((1, 20, 40, 16))\n",
    "\n",
    "dX = max_pool_naive_backward(dY, X, S)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf_X = tf.constant(X)\n",
    "    tf_Y = tf.nn.max_pool(\n",
    "        tf_X,\n",
    "        ksize=[1, S, S, 1],\n",
    "        strides=[1, S, S, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    tf_L = tf.reduce_sum(tf_Y)\n",
    "    tf_grad = tf.gradients(tf_L, [tf_X])\n",
    "    tf_dX, = sess.run(tf_grad)\n",
    "\n",
    "print(\"=== Matched? ===\")    \n",
    "check = np.linalg.norm(dX - tf_dX) / ((np.linalg.norm(dX) + np.linalg.norm(tf_dX)))\n",
    "print(check < 1e-7, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
